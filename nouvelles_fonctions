Idées de fonctionnalités à ajouter (inspirées de Strava, Garmin, etc.) pour mieux préparer une course.

Analyse et qualité des données réelles
- Détection des meilleurs temps (1 km, 5 km, 10 km, semi, marathon) sur une sortie et historique de progression.
- Distribution des pentes et des allures (histogrammes) + zones de vitesse/cadence/FC/power si données dispo.

Préparation théorique et pacing sur données théoriques 
- Simulation de départ négatif/positif (negative split vs positive split) et impact sur le temps final estimé.
- Plan de pacing par segments : diviser le parcours en portions (montées, descentes, plats) et proposer une allure cible par portion.
- Ajustement par conditions météo/altitude/surface : modulateur de l'allure de base selon température, humidité, vent, altitude, type de sol.
- Plan d'hydratation/nutrition : timeline (km ou minutes) avec rappels gels/boisson; exportable.

Visualisation avancée
- Table et courbe de "temps cumulé vs distance" avec curseur pour voir l'avance/retard par rapport à une cible.
- Carte interactive avec heatmap d'allure ou de fréquence cardiaque si disponible; points d'eau/ravitaillement épinglés.
- Mode recon : vue 3D du profil (pydeck Deck.gl TerrainLayer) ou mini roadbook (liste des côtes avec distance/pente/longueur).

Optimisations : 
- Cache Streamlit : mémoïser les étapes coûteuses (lecture/parse GPX, construction du DataFrame, calculs d’allure/pente/GAP, histos/figures) avec st.cache_data (résultats) et st.cache_resource (objets lourds). Éviter de recalculer quand seuls des sliders de visualisation changent.
- Partager les dérivés : calculer une fois les colonnes dérivées (pente lissée, moving_mask, pace lissé, GAP, bins) et les réutiliser dans tous les graphes. Actuellement plusieurs fonctions recalculent la pente et filtrent le mouvement.
- Déplacer le lissage : si les sliders ne changent que le rendu (ex. cap, smoothing), appliquer ces transformations sur des séries déjà prêtes plutôt que de refaire les stats de base (pente/pace) à chaque interaction.

Calculs répétitifs qu'on peut regroupper : 
- Pente lissée (fenêtre 5) : aujourd’hui recalculée dans compute_grade_percent_series, compute_gap_series, build_pace_grade_scatter, build_pace_grade_heatmap, build_residuals_vs_grade, compute_climbs → 6 fois → 1 seule série pré-calculée et réutilisée.
- Masque “en mouvement” : appelé dans compute_summary_stats, dans le calcul du temps en mouvement (slider), et dans compute_pause_markers → 3 fois → 1 seul calcul partagé.
- Agrégations allure vs pente par bin (médiane/écart-type) : faites dans plusieurs graphes (courbe lissée, résidus, éventuellement heatmap/scatter en filtrant) → 2-3 groupby/binnings à fusionner en 1 jeu d’agrégats réutilisé.
- Lecture/parse GPX + DataFrame de base : 1 parse par fichier et non à chaque interaction (via cache).

Ajout d'une courbe pro dans allure vs pente (chercher les données avec ce prompt sur chatgpt) : 
Contexte app : - App Streamlit d’analyse de traces GPX (stats, carte, allure vs pente, heatmap, résidus, GAP, etc.). Objectif : - Ajouter au graphique “Allure en fonction de la pente” une courbe de référence “coureur pro” en pointillés pour comparaison. Ta tâche : - Fournis une table réaliste allure vs pente pour un coureur élite route (allure plat ~3:20 min/km ≈ 200 s/km). - Plage de pentes : -10 % à +15 % avec pas de 1 %. - Colonnes : grade_percent (float), pace_s_per_km_pro (float, s/km). - Comportement réaliste : plus rapide en descente modérée, ralentissement progressif en montée, transition lissée autour de 0 %. - Format de réponse STRICT : CSV avec en-tête, aucune prose ni commentaire. Exemple de format (ne pas réutiliser ces chiffres) : grade_percent,pace_s_per_km_pro -10,170 -9,172 ... 0,200 ... +15,310